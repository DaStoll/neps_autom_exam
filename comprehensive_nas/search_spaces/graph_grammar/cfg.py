import itertools
import sys
from collections import defaultdict, deque
from typing import Tuple

import numpy as np
from nltk import CFG
from nltk.grammar import Nonterminal


class Grammar(CFG):
    """
    Extended context free grammar (CFG) class from the NLTK python package
    We have provided functionality to sample from the CFG.
    We have included generation capability within the class (before it was an external function)
    Also allow sampling to return whole trees (not just the string of terminals)
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # store some extra quantities needed later
        non_unique_nonterminals = [str(prod.lhs()) for prod in self.productions()]
        self.nonterminals = list(set(non_unique_nonterminals))
        self.terminals = list(
            {str(individual) for prod in self.productions() for individual in prod.rhs()}
            - set(self.nonterminals)
        )
        # collect nonterminals that are worth swapping when doing genetic operations (i.e not those with a single production that leads to a terminal)
        self.swappable_nonterminals = list(
            {i for i in non_unique_nonterminals if non_unique_nonterminals.count(i) > 1}
        )

        self.max_sampling_level = 2

        self.convergent = False

        self.check_grammar()

    def set_convergent(self):
        self.convergent = True

    def set_unconstrained(self):
        self.convergent = False

    def check_grammar(self):
        if len(set(self.terminals).intersection(set(self.nonterminals))) > 0:
            raise Exception(
                f"Same terminal and nonterminal symbol: {set(self.terminals).intersection(set(self.nonterminals))}!"
            )
        for nt in self.nonterminals:
            if len(self.productions(Nonterminal(nt))) == 0:
                raise Exception(f"There is no production for nonterminal {nt}")

    def generator(self, n=1, depth=5):
        # return the first n strings generated by the CFG of a maximum depth
        sequences = []
        for sentence in self._generate(n=n, depth=depth):
            sequences.append(" ".join(sentence))
        return sequences

    def sampler_restricted(self, n, max_length=5, cfactor=0.1, min_length=0):
        # sample n unqiue sequences from the CFG
        # such that the number of terminals is between min_length and max_length
        # cfactor controls the avg length of sampled sequence (see self.sampler)
        # setting smaller cfactor can reduce number of samples required to find n of specified size

        # store in a dict fr quick look up when seeing if its a unique sample
        sequences_dict = {}
        sequences = [[]] * n
        i = 0
        while i < n:
            sample = self._convergent_sampler(symbol=self.start(), cfactor=cfactor)
            # split up words, depth and num productions
            tree = sample[0] + ")"
            # count number of terminals
            length = 0
            for t in self.terminals:
                length += tree.count(t + ")")
            # check satisfies depth restrictions
            if (length <= max_length) and (length >= min_length):
                # check not already in samples
                if tree not in sequences_dict:
                    sequences_dict[tree] = "true"
                    sequences[i] = tree
                    i += 1
        return sequences

    def sampler(
        self,
        n=1,
        cfactor=0.1,
        start_symbol: str = None,
    ):
        # sample n sequences from the CFG
        # convergent: avoids very long sequences (we advise setting True)
        # cfactor: the factor to downweight productions (cfactor=1 returns to naive sampling strategy)
        #          smaller cfactor provides smaller sequences (on average)

        # Note that a simple recursive traversal of the grammar (setting convergent=False) where we choose
        # productions at random, often hits Python's max recursion depth as the longer a sequnce gets, the
        # less likely it is to terminate. Therefore, we set the default sampler (setting convergent=True) to
        # downweight frequent productions when traversing the grammar.
        # see https://eli.thegreenplace.net/2010/01/28/generating-random-sentences-from-a-context-free-236grammar
        if start_symbol is None:
            start_symbol = self.start()
        else:
            start_symbol = Nonterminal(start_symbol)

        if self.convergent:
            return [
                f"{self._convergent_sampler(symbol=start_symbol, cfactor=cfactor)[0]})"
                for i in range(0, n)
            ]
        else:
            return [f"{self._sampler(symbol=start_symbol)})" for i in range(0, n)]

    def _sampler(self, symbol=None):
        # simple sampler where each production is sampled uniformly from all possible productions
        # Tree choses if return tree or list of terminals
        # recursive implementation

        # init the sequence
        tree = "(" + str(symbol)
        # collect possible productions from the starting symbol
        productions = self.productions(lhs=symbol)
        # sample
        production = choice(productions)
        for sym in production.rhs():
            if isinstance(sym, str):
                # if terminal then add string to sequence
                tree = tree + " " + sym
            else:
                tree = tree + " " + self._sampler(sym) + ")"
        return tree

    def sampler_maxMin_func(self, symbol: str = None, largest: bool = True):
        tree = "(" + str(symbol)
        # collect possible productions from the starting symbol
        productions = self.productions(lhs=symbol)
        # sample
        production = productions[-1 if largest else 0]
        for sym in production.rhs():
            if isinstance(sym, str):
                # if terminal then add string to sequence
                tree = tree + " " + sym
            else:
                tree = tree + " " + self.sampler_maxMin_func(sym, largest=largest) + ")"
        return tree

    def _convergent_sampler(
        self, cfactor, symbol=None, pcount=defaultdict(int)
    ):  # pylint: disable=dangerous-default-value
        # sampler that down-weights the probability of selcting the same production many times
        # ensuring that the sampled trees are not 'too' long (size to be controlled by cfactor)
        #
        # recursive implementation
        #:pcount: storage for the productions used in the current branch

        # init the sequence
        tree = "(" + str(symbol)
        # init counter of tree depth and number of production rules
        depth, num_prod = 1, 1
        # collect possible productions from the starting symbol
        productions = self.productions(lhs=symbol)
        # init sampling weights
        weights = []
        # calc weights for the possible productions
        for prod in productions:
            if prod in pcount:
                # if production already occured in branch then downweight
                weights.append(cfactor ** (pcount[prod]))
            else:
                # otherwise set to be 1
                weights.append(1.0)
        # normalize weights to get probabilities
        norm = sum(weights)
        probs = [weight / norm for weight in weights]
        # sample
        production = choice(productions, probs)
        # update counts
        pcount[production] += 1
        depths = []
        for sym in production.rhs():
            if isinstance(sym, str):
                # if terminal then add string to sequence
                tree = tree + " " + sym
            else:
                # otherwise keep generating the sequence
                recursion = self._convergent_sampler(
                    symbol=sym, cfactor=cfactor, pcount=pcount
                )
                depths.append(recursion[1])
                num_prod += recursion[2]
                tree = tree + " " + recursion[0] + ")"
        # count the maximum depth and update

        if len(depths) > 0:
            depth = max(depths) + 1
        # update counts
        pcount[production] -= 1
        return tree, depth, num_prod

    def _generate(self, start=None, depth=None, n=None):
        """
        see https://www.nltk.org/_modules/nltk/parse/generate.html
        Generates an iterator of all sentences from a CFG.

        :param grammar: The Grammar used to generate sentences.
        :param start: The Nonterminal from which to start generate sentences.
        :param depth: The maximal depth of the generated tree.
        :param n: The maximum number of sentences to return.
        :return: An iterator of lists of terminal tokens.
        """
        if not start:
            start = self.start()
        if depth is None:
            depth = sys.maxsize

        iter_prod = self._generate_all([start], depth)

        if n:
            iter_prod = itertools.islice(iter_prod, n)

        return iter_prod

    def _generate_all(self, items, depth):
        # see https://www.nltk.org/_modules/nltk/parse/generate.html
        if items:
            try:
                for frag1 in self._generate_one(items[0], depth):
                    for frag2 in self._generate_all(items[1:], depth):
                        yield frag1 + frag2
            except RuntimeError as _error:
                if _error.message == "maximum recursion depth exceeded":
                    # Helpful error message while still showing the recursion stack.
                    raise RuntimeError(
                        "The grammar has rule(s) that yield infinite recursion!!"
                    )
                else:
                    raise
        else:
            yield []

    def _generate_one(self, item, depth):
        # see https://www.nltk.org/_modules/nltk/parse/generate.html
        if depth > 0:
            if isinstance(item, Nonterminal):
                for prod in self.productions(lhs=item):
                    for frag in self._generate_all(prod.rhs(), depth - 1):
                        yield frag
            else:
                yield [item]

    def mutate(
        self, parent: str, subtree_index: int, subtree_node: str, patience: int = 50
    ) -> str:
        # chop out subtree
        pre, _, post = self.remove_subtree(parent, subtree_index)
        _patience = patience
        while _patience > 0:
            # only sample subtree -> avoids full sampling of large parse trees
            new_subtree = self.sampler(1, start_symbol=subtree_node)[0]
            child = pre + new_subtree + post
            if parent != child:  # ensure that parent is really mutated
                break
            _patience -= 1
        return child

    def crossover(self, parent1: str, parent2: str, patience: int = 50):
        # randomly swap subtrees in two trees
        # if no suitiable subtree exists then return False
        subtree_node, subtree_index = self.rand_subtree(parent1)
        # chop out subtree
        pre, sub, post = self.remove_subtree(parent1, subtree_index)
        _patience = patience
        while _patience > 0:
            # sample subtree from donor
            donor_subtree_index = self.rand_subtree_fixed_head(parent2, subtree_node)
            # if no subtrees with right head node return False
            if not donor_subtree_index:
                _patience -= 1
            else:
                donor_pre, donor_sub, donor_post = self.remove_subtree(
                    parent2, donor_subtree_index
                )
                # return the two new tree
                child1 = pre + donor_sub + post
                child2 = donor_pre + sub + donor_post
                return child1, child2

        return False, False

    def rand_subtree(self, tree: str) -> Tuple[str, int]:
        # helper function to choose a random subtree in a given tree
        # returning the parent node of the subtree and its index
        # single pass through tree (stored as string) to look for the location of swappable_non_terminmals
        split_tree = tree.split(" ")
        swappable_indices = [
            i
            for i in range(0, len(split_tree))
            if split_tree[i][1:] in self.swappable_nonterminals
        ]
        # randomly choose one of these non-terminals to replace its subtree
        r = np.random.randint(1, len(swappable_indices))
        chosen_non_terminal = split_tree[swappable_indices[r]][1:]
        chosen_non_terminal_index = swappable_indices[r]
        # return chosen node and its index
        return chosen_non_terminal, chosen_non_terminal_index

    @staticmethod
    def rand_subtree_fixed_head(
        tree: str, head_node: str, swappable_indices: list = None
    ) -> int:
        # helper function to choose a random subtree from a given tree with a specific head node
        # if no such subtree then return False, otherwise return the index of the subtree

        # single pass through tree (stored as string) to look for the location of swappable_non_terminmals
        if swappable_indices is None:
            split_tree = tree.split(" ")
            swappable_indices = [
                i for i in range(0, len(split_tree)) if split_tree[i][1:] == head_node
            ]
        if not isinstance(swappable_indices, list):
            raise TypeError("Expected list for swappable indices!")
        if len(swappable_indices) == 0:
            # no such subtree
            return False
        else:
            # randomly choose one of these non-terminals
            r = (
                np.random.randint(1, len(swappable_indices))
                if len(swappable_indices) > 1
                else 0
            )
            chosen_non_terminal_index = swappable_indices[r]
            return chosen_non_terminal_index

    @staticmethod
    def remove_subtree(tree, index) -> Tuple[str, str, str]:
        # helper function to remove a subtree from a tree (given its index)
        # returning the str before and after the subtree
        # i.e '(S (S (T 2)) (ADD +) (T 1))'
        # becomes '(S (S (T 2)) ', '(T 1))'  after removing (ADD +)

        split_tree = tree.split(" ")
        pre_subtree = " ".join(split_tree[:index]) + " "
        #  get chars to the right of split
        right = " ".join(split_tree[index + 1 :])
        # remove chosen subtree
        # single pass to find the bracket matching the start of the split
        counter, current_index = 1, 0
        for char in right:
            if char == "(":
                counter += 1
            elif char == ")":
                counter -= 1
            if counter == 0:
                break
            current_index += 1
        # retrun string after remover tree
        post_subtree = right[current_index + 1 :]
        # get removed tree
        removed = "".join(split_tree[index]) + " " + right[: current_index + 1]
        return (pre_subtree, removed, post_subtree)


class DepthConstrainedGrammar(Grammar):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.depth_constraints = None

    def set_depth_constraints(self, depth_constraints):
        self.depth_constraints = depth_constraints
        if not all(k in self.nonterminals for k in self.depth_constraints.keys()):
            raise Exception(
                f"Nonterminal {set(self.depth_constraints.keys())-set(self.nonterminals)} does not exist in grammar"
            )

    @staticmethod
    def is_depth_constrained():
        return True

    def sampler(
        self,
        n=1,
        depth_information: dict = None,
        start_symbol: str = None,
    ):
        if self.depth_constraints is None:
            raise ValueError("Depth constraints are not set!")

        if start_symbol is None:
            start_symbol = self.start()
        else:
            start_symbol = Nonterminal(start_symbol)

        if depth_information is None:
            depth_information = {}
        return [
            f"{self._depth_constrained_sampler(symbol=start_symbol, depth_information=depth_information)})"
            for i in range(0, n)
        ]

    def _compute_depth_information_for_pre(self, tree: str) -> dict:
        depth_information = {nt: 0 for nt in self.nonterminals}
        q_nonterminals = deque()
        for split in tree.split(" "):
            if split == "":
                continue
            elif split[0] == "(":
                q_nonterminals.append(split[1:])
                depth_information[split[1:]] += 1
                continue
            while split[-1] == ")":
                nt = q_nonterminals.pop()
                depth_information[nt] -= 1
                split = split[:-1]
        return depth_information

    def _compute_depth_information(self, tree: str) -> list:
        split_tree = tree.split(" ")
        depth_information = [0] * len(split_tree)
        helper_dict = {nt: 0 for nt in self.nonterminals}
        q_nonterminals = deque()
        for i, split in enumerate(split_tree):
            if split == "":
                continue
            elif split[0] == "(":
                q_nonterminals.append(split[1:])
                depth_information[i] = helper_dict[split[1:]] + 1
                helper_dict[split[1:]] += 1
                continue
            while split[-1] == ")":
                nt = q_nonterminals.pop()
                helper_dict[nt] -= 1
                split = split[:-1]
        return depth_information

    def _depth_constrained_sampler(self, symbol=None, depth_information: dict = None):
        if depth_information is None:
            depth_information = {}
        # init the sequence
        tree = "(" + str(symbol)
        # collect possible productions from the starting symbol & filter if constraints are violated
        lhs = str(symbol)
        if lhs in depth_information.keys():
            depth_information[lhs] += 1
        else:
            depth_information[lhs] = 1
        if (
            lhs in self.depth_constraints.keys()
            and depth_information[lhs] > self.depth_constraints[lhs]
            # and depth_information[lhs] >= self.depth_constraints[lhs] # new design
        ):
            productions = [
                production
                for production in self.productions(lhs=symbol)
                if lhs
                not in [str(sym) for sym in production.rhs() if not isinstance(sym, str)]
            ]
        else:
            productions = self.productions(lhs=symbol)

        if len(productions) == 0:
            raise Exception(
                f"There can be no word sampled! This is due to the grammar and/or constraints."
            )

        # sample
        production = choice(productions)
        for sym in production.rhs():
            if isinstance(sym, str):
                # if terminal then add string to sequence
                tree = tree + " " + sym
            else:
                tree = (
                    tree
                    + " "
                    + self._depth_constrained_sampler(sym, depth_information)
                    + ")"
                )
        depth_information[lhs] -= 1
        return tree

    def mutate(
        self, parent: str, subtree_index: int, subtree_node: str, patience: int = 50
    ) -> str:
        # chop out subtree
        pre, _, post = self.remove_subtree(parent, subtree_index)
        _patience = patience
        while _patience > 0:
            # only sample subtree -> avoids full sampling of large parse trees
            depth_information = self._compute_depth_information_for_pre(pre)
            new_subtree = self.sampler(
                1, start_symbol=subtree_node, depth_information=depth_information
            )[0]
            child = pre + new_subtree + post
            if parent != child:  # ensure that parent is really mutated
                break
            _patience -= 1
        return child

    def crossover(self, parent1: str, parent2: str, patience: int = 50):
        # randomly swap subtrees in two trees
        # if no suitiable subtree exists then return False
        subtree_node, subtree_index = self.rand_subtree(parent1)
        # chop out subtree
        pre, sub, post = self.remove_subtree(parent1, subtree_index)
        _patience = patience
        while _patience > 0:
            # sample subtree from donor
            head_node_constraint = self._compute_depth_information_for_pre(pre)[
                subtree_node
            ]
            donor_subtree_index = self._rand_subtree_fixed_head(
                parent2, subtree_node, head_node_constraint
            )
            # if no subtrees with right head node return False
            if not donor_subtree_index:
                _patience -= 1
            else:
                donor_pre, donor_sub, donor_post = self.remove_subtree(
                    parent2, donor_subtree_index
                )
                # return the two new tree
                child1 = pre + donor_sub + post
                child2 = donor_pre + sub + donor_post
                return child1, child2

        return False, False

    def _rand_subtree_fixed_head(
        self, tree: str, head_node: str, head_node_depth_constraint: int = 0
    ) -> int:
        # helper function to choose a random subtree from a given tree with a specific head node
        # if no such subtree then return False, otherwise return the index of the subtree

        # single pass through tree (stored as string) to look for the location of swappable_non_terminmals
        split_tree = tree.split(" ")
        depth_information = self._compute_depth_information(tree)
        swappable_indices = [
            i
            for i in range(0, len(split_tree))
            if split_tree[i][1:] == head_node
            and depth_information[i] >= head_node_depth_constraint
        ]
        return super().rand_subtree_fixed_head(
            tree=tree, head_node=head_node, swappable_indices=swappable_indices
        )


class ConstrainedGrammar(Grammar):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.constraints = None

    def set_constraints(self, constraints: dict):
        self.constraints = constraints

    @staticmethod
    def is_constrained():
        return True

    def sampler(
        self,
        n=1,
        start_symbol: str = None,
        not_allowed_productions=None,
    ):
        if start_symbol is None:
            start_symbol = self.start()
        else:
            start_symbol = Nonterminal(start_symbol)

        return [
            self._constrained_sampler(
                symbol=start_symbol,
                not_allowed_productions=not_allowed_productions,
            )
            + ")"
            for _ in range(n)
        ]

    def _constrained_sampler(
        self,
        symbol=None,
        not_allowed_productions=None,
        current_derivation=None,
    ):
        # simple sampler where each production is sampled uniformly from all possible productions
        # Tree choses if return tree or list of terminals
        # recursive implementation

        # init the sequence
        tree = "(" + str(symbol)
        # collect possible productions from the starting symbol
        productions = self.productions(lhs=symbol)
        if not_allowed_productions is not None:
            productions = list(
                filter(
                    lambda production: production not in not_allowed_productions,
                    productions,
                )
            )
        # sample
        production = choice(productions)
        counter = 0
        current_derivation = self.constraints(production.rhs()[0])
        for sym in production.rhs():
            if isinstance(sym, str):
                # if terminal then add string to sequence
                tree = tree + " " + sym
            else:
                not_allowed_productions = self.constraints(
                    production.rhs()[0],
                    self.productions(lhs=sym),
                    current_derivation,
                )
                ret_val = self._constrained_sampler(sym, not_allowed_productions)
                tree = tree + " " + ret_val + ")"
                current_derivation[counter] = ret_val
                counter += 1
        return tree

    def _compute_current_context(self, pre_subtree: str):
        q_nonterminals = deque()
        for sym in pre_subtree.split(" "):
            if sym == "":
                continue
            if sym[0] == "(":
                sym = sym[1:]
            if sym[-1] == ")":
                for _ in range(sym.count(")")):
                    q_nonterminals.pop()
                while sym[-1] == ")":
                    sym = sym[:-1]
            if len(sym) == 1 and sym[0] in [" ", "\t", "\n", "[", "]"]:
                continue
            if sym in self.nonterminals:
                q_nonterminals.append(sym)

        context_start_idx = pre_subtree.rfind(q_nonterminals[-1])
        pre_subtree_context = pre_subtree[context_start_idx:]
        topology = pre_subtree_context[len(q_nonterminals[-1]) + 1 :].split(" ")[0]
        productions = [
            prod
            for prod in self.productions()
            if pre_subtree_context[: len(q_nonterminals[-1])] == f"{prod.lhs()}"
            and prod.rhs()[0] == topology
        ]
        if len(productions) == 0:
            raise Exception("Cannot find corresponding production!")
        if len(productions) > 1:
            raise NotImplementedError
        production = productions[0]

        q_context = deque()
        production_nonterminals = [
            prod for prod in production.rhs() if str(prod) in self.nonterminals
        ]
        current_derivation = self.constraints(production.rhs()[0])
        counter = 0
        pre_subtree_context = pre_subtree_context[len(q_nonterminals[-1]) + 1 :]
        for i, s in enumerate(pre_subtree_context):
            if s == "(":
                start_idx = i
                q_context.append(i)
            elif s == ")":
                q_context.pop()
                if len(q_context) == 0:
                    end_idx = i
                    full_context = pre_subtree_context[start_idx:end_idx]
                    ret_val = full_context[full_context.find(" ") + 1 :]
                    current_derivation[counter] = ret_val
                    counter += 1

        return self.constraints(
            production.rhs()[0],
            self.productions(lhs=production_nonterminals[counter]),
            current_derivation,
        )

    def mutate(
        self, parent: str, subtree_index: int, subtree_node: str, patience: int = 50
    ):
        # chop out subtree
        pre, _, post = self.remove_subtree(parent, subtree_index)
        not_allowed_productions = self._compute_current_context(pre)
        _patience = patience
        while _patience > 0:
            # only sample subtree -> avoids full sampling of large parse trees
            new_subtree = self.sampler(
                1,
                start_symbol=subtree_node,
                not_allowed_productions=not_allowed_productions,
            )[0]
            child = pre + new_subtree + post
            if parent != child:  # ensure that parent is really mutated
                break
            _patience -= 1
        return child

    def crossover(self, parent1: str, parent2: str, patience: int = 50):
        subtree_node, subtree_index = self.rand_subtree(parent1)
        # chop out subtree
        pre, sub, post = self.remove_subtree(parent1, subtree_index)
        parent1_not_allowed_productions = self._compute_current_context(pre)
        _patience = patience
        while _patience > 0:
            # sample subtree from donor
            donor_subtree_index = self.rand_subtree_fixed_head(parent2, subtree_node)
            # if no subtrees with right head node return False
            if not donor_subtree_index:
                _patience -= 1
            else:
                donor_pre, donor_sub, donor_post = self.remove_subtree(
                    parent2, donor_subtree_index
                )
                parent2_not_allowed_productions = self._compute_current_context(donor_pre)
                if (
                    parent1_not_allowed_productions is not None
                    and "zero" in donor_sub
                    and donor_sub.count("(") == 1
                    and donor_sub.count(")") == 1
                ):
                    _patience -= 1
                    continue
                if (
                    parent2_not_allowed_productions is not None
                    and "zero" in sub
                    and sub.count("(") == 1
                    and sub.count(")") == 1
                ):
                    _patience -= 1
                    continue
                # return the two new tree
                child1 = pre + donor_sub + post
                child2 = donor_pre + sub + donor_post
                return child1, child2

        return False, False


# helper function for quickly getting a single sample from multinomial with probs
def choice(options, probs=None):
    x = np.random.rand()
    if probs is None:
        # then uniform probs
        num = len(options)
        probs = [1 / num] * num
    cum = 0
    choice = -1
    for i, p in enumerate(probs):
        cum += p
        if x < cum:
            choice = i
            break
    return options[choice]


if __name__ == "__main__":
    import os

    import matplotlib.pyplot as plt
    import networkx as nx
    from path import Path

    from comprehensive_nas.search_spaces.graph_grammar.graph_grammar import GraphGrammar

    g = GraphGrammar([])

    dir_path = Path(os.path.dirname(os.path.realpath(__file__)))

    # simple arithmetic grammar
    search_space_path = dir_path / ".." / "debug_grammars" / "simple_arithmetic.cfg"
    with open(search_space_path) as f:
        productions = f.read()

    grammar = Grammar.fromstring(productions)
    # sample a short sequences
    tree = grammar.sampler(1, cfactor=0.0001)
    print(tree)
    nxTree = g.from_stringTree_to_nxTree(tree[0], grammar, sym_name="sym")
    # nx.nx_agraph.write_dot(nxTree,'/home/schrodi/hierarchical_nas_benchmarks/test.dot')
    # pos=graphviz_layout(nxTree, prog='dot')
    nx.draw(
        nxTree, with_labels=True, labels={k: v["sym"] for k, v in nxTree.nodes.items()}
    )
    plt.savefig("/home/schrodi/hierarchical_nas_benchmarks/test.png")
    plt.close()
    # print first sequences of depth 10
    print(grammar.generator(1, 10))
    # print the allowed productions
    print(grammar.productions())

    # SMILES grammar
    search_space_path = dir_path / ".." / "debug_grammars" / "SMILES.cfg"
    with open(search_space_path) as f:
        productions = f.read()
    grammar = Grammar.fromstring(productions)
    # sample a short sequences
    print(grammar.sampler(1, cfactor=0.0001))
    # print first  sequences of depth 10
    print(grammar.generator(1, 10))
    # print the allowed productions
    print(grammar.productions())
